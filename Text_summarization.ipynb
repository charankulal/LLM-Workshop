{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Packages"
      ],
      "metadata": {
        "id": "C6mhuB8l1Xel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmkSYOFe7ILR",
        "outputId": "e449886f-67f2-48bf-e9a3-f58b30b22427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "lrwudbV41k5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5Tokenizer\n",
        "from datasets import load_dataset\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import os\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "ZEfT4MxT1pFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 1 : Data Collection"
      ],
      "metadata": {
        "id": "Ai8G09sd1seV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:1000]\")\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "xo63efiF2LFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 2: Data Preprocessing"
      ],
      "metadata": {
        "id": "WvB-HuPj2RoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SummarizationDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_input_length=256, max_target_length=128):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_input_length = max_input_length\n",
        "        self.max_target_length = max_target_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        article = self.dataset[idx][\"article\"]\n",
        "        summary = self.dataset[idx][\"highlights\"]\n",
        "\n",
        "        article = \"summarize: \" + article\n",
        "\n",
        "        input_encoding = self.tokenizer(\n",
        "            article,\n",
        "            max_length=self.max_input_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        target_encoding = self.tokenizer(\n",
        "            summary,\n",
        "            max_length=self.max_target_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_encoding[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": input_encoding[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": target_encoding[\"input_ids\"].squeeze()\n",
        "        }\n"
      ],
      "metadata": {
        "id": "OtdMerVz3H7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 3 : Model Building"
      ],
      "metadata": {
        "id": "_rCaLw0J4LSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, nhead=4, num_encoder_layers=3, num_decoder_layers=3, dim_feedforward=1024, dropout=0.1):\n",
        "        super(CustomTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, 512, d_model))  # Increased to 512\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        src = self.embedding(src) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float))\n",
        "        src = src + self.pos_encoder[:, :src.size(1), :]\n",
        "        tgt = self.embedding(tgt) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float))\n",
        "        tgt = tgt + self.pos_encoder[:, :tgt.size(1), :]\n",
        "\n",
        "        output = self.transformer(src.transpose(0, 1), tgt.transpose(0, 1), src_mask, tgt_mask)\n",
        "        output = self.fc_out(output)\n",
        "        return output.transpose(0, 1)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n"
      ],
      "metadata": {
        "id": "3tXRIeEO4K03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_tokenizer():\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "    model = CustomTransformer(vocab_size=tokenizer.vocab_size)\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "ifOuS8ZF4ayO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 4 : Model Training"
      ],
      "metadata": {
        "id": "BXPp59yW502O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, tokenizer, epochs=1, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            decoder_input = labels[:, :-1]\n",
        "            decoder_target = labels[:, 1:]\n",
        "\n",
        "            tgt_mask = model.generate_square_subsequent_mask(decoder_input.size(1)).to(device)\n",
        "\n",
        "            with autocast():\n",
        "                output = model(input_ids, decoder_input, tgt_mask=tgt_mask)\n",
        "                loss = criterion(output.reshape(-1, output.size(-1)), decoder_target.reshape(-1))\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "    # Save model and tokenizer\n",
        "    torch.save(model.state_dict(), \"summarizer_custom_model.pt\")\n",
        "    os.makedirs(\"summarizer_custom_model\", exist_ok=True)\n",
        "    tokenizer.save_pretrained(\"summarizer_custom_model\")"
      ],
      "metadata": {
        "id": "FebXMnTU55_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 5 : Summarization"
      ],
      "metadata": {
        "id": "W5rGVFfv6FYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(model, tokenizer, text, max_length=128, min_length=30, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    text = \"summarize: \" + text\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        max_length=256,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "\n",
        "    generated_ids = beam_search(model, tokenizer, input_ids, max_length, min_length, device)\n",
        "    summary = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "    return summary\n",
        "\n"
      ],
      "metadata": {
        "id": "pct4REDd6JBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(model, tokenizer, input_ids, max_length, min_length, device, beam_size=4):\n",
        "    model.eval()\n",
        "    sequences = [(input_ids, 0.0)]\n",
        "    for step in range(max_length):\n",
        "        all_candidates = []\n",
        "        for seq, score in sequences:\n",
        "            decoder_input = seq[:, -1:].to(device) if step == 0 else seq[:, 1:].to(device)\n",
        "            tgt_mask = model.generate_square_subsequent_mask(decoder_input.size(1)).to(device)\n",
        "            output = model(input_ids, decoder_input, tgt_mask=tgt_mask)\n",
        "            log_probs = torch.log_softmax(output[:, -1, :], dim=-1)\n",
        "            topk_log_probs, topk_ids = log_probs.topk(beam_size)\n",
        "\n",
        "            for i in range(beam_size):\n",
        "                candidate_seq = torch.cat([seq, topk_ids[:, i].unsqueeze(1)], dim=1)\n",
        "                candidate_score = score - topk_log_probs[0, i].item()\n",
        "                all_candidates.append((candidate_seq, candidate_score))\n",
        "\n",
        "        sequences = sorted(all_candidates, key=lambda x: x[1])[:beam_size]\n",
        "\n",
        "        if step >= min_length and all(tokenizer.eos_token_id in seq[0] for seq, _ in sequences):\n",
        "            break\n",
        "\n",
        "    return sequences[0][0][0]"
      ],
      "metadata": {
        "id": "g33S1F5X6RCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 6: Execution"
      ],
      "metadata": {
        "id": "Ta1mhA096eJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load data\n",
        "    dataset = load_data()\n",
        "\n",
        "    # Initialize model and tokenizer\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "    # Preprocess data\n",
        "    train_dataset = SummarizationDataset(dataset, tokenizer)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "    # Train model\n",
        "    train_model(model, train_dataloader, tokenizer)\n",
        "\n",
        "    # Test summarization\n",
        "    sample_text = \"\"\"\n",
        "    The quick brown fox jumps over the lazy dog. This is a classic pangram used to test typewriters and keyboards.\n",
        "    It contains every letter of the English alphabet. The fox is known for its agility and cunning, while the dog,\n",
        "    in this case, is depicted as idle. This sentence has been used in various contexts to demonstrate text processing.\n",
        "    The pangram is often employed in design and development to ensure that fonts and text rendering systems display\n",
        "    all characters correctly. Its brevity and inclusivity make it a practical tool for testing.\n",
        "    \"\"\"\n",
        "    summary = summarize_text(model, tokenizer, sample_text)\n",
        "    print(\"Original Text:\", sample_text)\n",
        "    print(\"Original Text Word Count:\", len(sample_text.split()))\n",
        "    print(\"Summary:\", summary)\n",
        "    print(\"Summary Word Count:\", len(summary.split()))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "U4ZoanmL6muz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a541d816-6089-4b24-a71c-ed99ba328d39"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-49-0f2141fe5d23>\", line 30, in <cell line: 0>\n",
            "    main()\n",
            "  File \"<ipython-input-49-0f2141fe5d23>\", line 13, in main\n",
            "    train_model(model, train_dataloader, tokenizer)\n",
            "  File \"<ipython-input-48-d6be2a9049b2>\", line 26, in train_model\n",
            "    scaler.scale(loss).backward()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 626, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n",
            "    _engine_run_backward(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 823, in _engine_run_backward\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1684, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 948, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 997, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "    ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen posixpath>\", line 412, in realpath\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-0f2141fe5d23>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-0f2141fe5d23>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-d6be2a9049b2>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, tokenizer, epochs, device)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Custom Model"
      ],
      "metadata": {
        "id": "L-lX6Tb8vEAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import T5Tokenizer\n",
        "import os\n",
        "\n",
        "class CustomTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, nhead=4, num_encoder_layers=3, num_decoder_layers=3, dim_feedforward=1024, dropout=0.1):\n",
        "        super(CustomTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, 256, d_model))  # Increased to 512\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        src = self.embedding(src) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float))\n",
        "        src = src + self.pos_encoder[:, :src.size(1), :]\n",
        "        tgt = self.embedding(tgt) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float))\n",
        "        tgt = tgt + self.pos_encoder[:, :tgt.size(1), :]\n",
        "\n",
        "        output = self.transformer(src.transpose(0, 1), tgt.transpose(0, 1), src_mask, tgt_mask)\n",
        "        output = self.fc_out(output)\n",
        "        return output.transpose(0, 1)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "\n",
        "def load_model_and_tokenizer(model_dir=\"summarizer_custom_model\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    if not os.path.exists(model_dir):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Model directory '{model_dir}' not found. Ensure you have run the training script \"\n",
        "            \"and the model was saved correctly.\"\n",
        "        )\n",
        "\n",
        "    try:\n",
        "        tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
        "        model = CustomTransformer(vocab_size=tokenizer.vocab_size)\n",
        "        model.load_state_dict(torch.load('/content/summarizer_model.pt', map_location=device))\n",
        "        model = model.to(device)\n",
        "        model.eval()\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error loading model or tokenizer: {str(e)}\")\n",
        "\n",
        "def summarize_text(model, tokenizer, text, max_length=128, min_length=30, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Add prefix\n",
        "    text = \"summarize: \" + text\n",
        "\n",
        "    # Tokenize input text\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        max_length=256,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "\n",
        "    generated_ids = beam_search(model, tokenizer, input_ids, max_length, min_length, device)\n",
        "    summary = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "def beam_search(model, tokenizer, input_ids, max_length, min_length, device, beam_size=4):\n",
        "    model.eval()\n",
        "    sequences = [(input_ids, 0.0)]\n",
        "    for step in range(max_length):\n",
        "        all_candidates = []\n",
        "        for seq, score in sequences:\n",
        "            decoder_input = seq[:, -1:].to(device) if step == 0 else seq[:, 1:].to(device)\n",
        "            tgt_mask = model.generate_square_subsequent_mask(decoder_input.size(1)).to(device)\n",
        "            output = model(input_ids, decoder_input, tgt_mask=tgt_mask)\n",
        "            log_probs = torch.log_softmax(output[:, -1, :], dim=-1)\n",
        "            topk_log_probs, topk_ids = log_probs.topk(beam_size)\n",
        "\n",
        "            for i in range(beam_size):\n",
        "                candidate_seq = torch.cat([seq, topk_ids[:, i].unsqueeze(1)], dim=1)\n",
        "                candidate_score = score - topk_log_probs[0, i].item()\n",
        "                all_candidates.append((candidate_seq, candidate_score))\n",
        "\n",
        "        sequences = sorted(all_candidates, key=lambda x: x[1])[:beam_size]\n",
        "\n",
        "        if step >= min_length and all(tokenizer.eos_token_id in seq[0] for seq, _ in sequences):\n",
        "            break\n",
        "\n",
        "    return sequences[0][0][0]\n",
        "\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        model, tokenizer = load_model_and_tokenizer()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return\n",
        "\n",
        "    sample_text = \"\"\"\n",
        "    ### **India: A Land of Diversity and Heritage**\n",
        "\n",
        "#### **Introduction**\n",
        "\n",
        "India, the seventh-largest country by land area and the second-most populous nation, is a land of immense diversity, rich history, and vibrant culture. Nestled in South Asia, India is bordered by Pakistan, China, Nepal, Bhutan, Bangladesh, and Myanmar, with the Indian Ocean to the south. It is known for its ancient civilization, vast landscapes, and contributions to global science, art, literature, and philosophy. From the towering Himalayas in the north to the serene backwaters of Kerala in the south, India is a nation that embodies unity in diversity.\n",
        "\n",
        "This essay explores India's history, geography, culture, economy, governance, challenges, and its role on the global stage.\n",
        "\n",
        "---\n",
        "\n",
        "## **Historical Background**\n",
        "\n",
        "### **Ancient India and Civilization**\n",
        "\n",
        "India has one of the world's oldest civilizations, dating back to the Indus Valley Civilization (3300–1300 BCE). The cities of Harappa and Mohenjo-Daro exemplify advanced urban planning, with well-structured drainage systems and standardized brick sizes. The Vedic Age (1500–500 BCE) followed, introducing Sanskrit literature, early Hinduism, and the caste system.\n",
        "\n",
        "During the Mauryan Empire (321–185 BCE), Emperor Ashoka played a significant role in spreading Buddhism across Asia. The Gupta Empire (4th–6th century CE) marked the golden age of Indian culture, advancing in mathematics (introduction of zero), astronomy, literature, and medicine.\n",
        "\n",
        "### **Medieval India**\n",
        "\n",
        "Medieval India witnessed the rise of regional kingdoms and the influence of Islam with the Delhi Sultanate (1206–1526) and later the Mughal Empire (1526–1857). The Mughal rulers, especially Akbar, encouraged religious tolerance and built architectural wonders like the Taj Mahal.\n",
        "\n",
        "### **Colonial Era and Independence Struggle**\n",
        "\n",
        "The British East India Company established control over India in the 18th century, exploiting its resources. The Revolt of 1857, also known as the First War of Independence, was the first major resistance against British rule. However, it was only in the 20th century, under the leadership of Mahatma Gandhi, Jawaharlal Nehru, Subhas Chandra Bose, and other freedom fighters, that India gained independence on **August 15, 1947**. The partition of India led to the creation of Pakistan, resulting in significant communal violence and mass migration.\n",
        "\n",
        "---\n",
        "\n",
        "## **Geography and Natural Resources**\n",
        "\n",
        "### **Diverse Terrain**\n",
        "\n",
        "India spans 3.28 million square kilometers and has a varied topography. The **Himalayas** form the northern boundary, acting as a natural defense and climate regulator. The **Gangetic plains** are the most fertile regions, supporting India's massive agricultural sector. The **Thar Desert** in Rajasthan and the **Deccan Plateau** in the south showcase the nation's geographic extremes.\n",
        "\n",
        "### **Rivers and Climate**\n",
        "\n",
        "Major rivers like the **Ganga, Yamuna, Brahmaputra, Godavari, Krishna, and Kaveri** are crucial for irrigation, hydroelectricity, and sustenance. India experiences diverse climatic conditions—tropical in the south, temperate in the north, and desert-like in the west.\n",
        "\n",
        "### **Natural Resources**\n",
        "\n",
        "India is rich in minerals such as coal, iron ore, bauxite, and petroleum. The country is also a leading producer of agricultural commodities like rice, wheat, spices, and cotton.\n",
        "\n",
        "---\n",
        "\n",
        "## **Cultural and Linguistic Diversity**\n",
        "\n",
        "### **Languages**\n",
        "\n",
        "India recognizes **22 official languages**, with Hindi and English serving as the primary means of communication. Over **1,600 dialects** are spoken, reflecting the nation’s linguistic diversity.\n",
        "\n",
        "### **Religious Pluralism**\n",
        "\n",
        "India is the birthplace of **Hinduism, Buddhism, Jainism, and Sikhism**. It is also home to large populations of **Muslims, Christians, Jews, and Zoroastrians**, demonstrating religious harmony despite occasional conflicts.\n",
        "\n",
        "### **Festivals and Traditions**\n",
        "\n",
        "Festivals like **Diwali, Holi, Eid, Christmas, Pongal, Baisakhi**, and **Navratri** symbolize India's cultural vibrancy. Weddings, traditional attire, and culinary diversity further enrich Indian heritage.\n",
        "\n",
        "### **Art and Literature**\n",
        "\n",
        "Indian classical music—**Hindustani and Carnatic**—along with dance forms like **Bharatanatyam, Kathak, Odissi, and Kathakali**, are globally recognized. Literature from ancient texts like the **Vedas, Upanishads, Mahabharata, and Ramayana** to modern works by **Rabindranath Tagore, R.K. Narayan, and Arundhati Roy** showcases India's intellectual richness.\n",
        "\n",
        "---\n",
        "\n",
        "## **Indian Economy: A Growing Global Power**\n",
        "\n",
        "### **Agriculture**\n",
        "\n",
        "India is one of the largest producers of **rice, wheat, sugarcane, pulses, and dairy products**. Despite its strong agrarian roots, challenges like small landholdings and unpredictable monsoons affect productivity.\n",
        "\n",
        "### **Industrial Growth**\n",
        "\n",
        "India's industrial sector, particularly **automobiles, textiles, steel, and pharmaceuticals**, has witnessed significant growth. The country is the world's largest vaccine producer and a hub for generic medicines.\n",
        "\n",
        "### **Information Technology and Services**\n",
        "\n",
        "India's IT industry, with giants like **TCS, Infosys, and Wipro**, contributes significantly to GDP. The outsourcing sector, particularly in **Bangalore, Hyderabad, and Pune**, has transformed India into a global IT powerhouse.\n",
        "\n",
        "### **Infrastructure and Space Exploration**\n",
        "\n",
        "India's infrastructure development includes **metros, highways, bullet trains, and smart cities**. The **Indian Space Research Organisation (ISRO)** has made remarkable strides, launching missions like **Chandrayaan (Moon) and Mangalyaan (Mars)** at record-low costs.\n",
        "\n",
        "---\n",
        "\n",
        "## **Government and Politics**\n",
        "\n",
        "India is the world's **largest democracy**, following a **parliamentary system**. The **President** is the constitutional head, while the **Prime Minister** holds executive power.\n",
        "\n",
        "### **Judiciary and Constitution**\n",
        "\n",
        "The **Indian Constitution**, adopted in 1950, is the world's longest. It guarantees fundamental rights, secularism, and social justice. The Supreme Court ensures legal protection for all citizens.\n",
        "\n",
        "### **Challenges in Governance**\n",
        "\n",
        "* **Corruption**: A major issue, though initiatives like Digital India and Aadhaar have improved transparency.\n",
        "* **Communalism and Casteism**: Though India is largely secular, caste-based and religious conflicts still occur.\n",
        "* **Unemployment and Poverty**: Economic reforms have reduced poverty, but income disparity remains high.\n",
        "\n",
        "---\n",
        "\n",
        "## **Global Influence and Foreign Policy**\n",
        "\n",
        "### **United Nations and International Relations**\n",
        "\n",
        "India is a key player in global politics, advocating **peace, non-alignment, and South-South cooperation**. It is a member of the **G20, BRICS, and United Nations Security Council (aspiring for a permanent seat)**.\n",
        "\n",
        "### **Defense and Nuclear Power**\n",
        "\n",
        "India has a strong military force and is a **nuclear power**, with a “No First Use” policy. The country is enhancing defense capabilities through projects like **Make in India**.\n",
        "\n",
        "### **Trade and Diplomacy**\n",
        "\n",
        "India has trade relations with **the USA, European Union, China, Russia, and neighboring countries**. The Indian diaspora in the **Middle East, North America, and the UK** significantly contributes to foreign remittances.\n",
        "\n",
        "---\n",
        "\n",
        "## **Challenges and the Road Ahead**\n",
        "\n",
        "### **Economic Inequality**\n",
        "\n",
        "India, despite being an emerging economy, has significant **wealth disparity**. Policies promoting rural development and skill enhancement can bridge the gap.\n",
        "\n",
        "### **Environmental Issues**\n",
        "\n",
        "Deforestation, air pollution, and water scarcity threaten India's sustainability. Renewable energy initiatives like **solar and wind power** are promising solutions.\n",
        "\n",
        "### **Education and Healthcare**\n",
        "\n",
        "Improving **public healthcare and education** is essential. The **National Education Policy (NEP 2020)** and **Ayushman Bharat scheme** aim to transform these sectors.\n",
        "\n",
        "---\n",
        "\n",
        "## **Conclusion**\n",
        "\n",
        "India, with its ancient legacy and modern aspirations, is on the path of progress. As a rapidly growing economy and a cultural powerhouse, India continues to play a crucial role on the world stage. By embracing **technology, education, sustainability, and social justice**, India can achieve unparalleled growth in the 21st century.\n",
        "\n",
        "India’s journey—from an ancient civilization to a modern global leader—remains an inspiring tale of resilience and ambition. 🚀\n",
        "\n",
        "---\n",
        "\n",
        "Would you like me to add references or more details on a specific section? 😊\n",
        "\n",
        "    \"\"\"\n",
        "    summary = summarize_text(model, tokenizer, sample_text)\n",
        "    # print(\"Original Text:\", sample_text)\n",
        "    print(\"Summary Word Count:\", len(sample_text.split()))\n",
        "    print(\"Summary:\", summary)\n",
        "    print(\"Summary Word Count:\", len(summary.split()))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-SnPAGGP39a",
        "outputId": "c834f989-76d3-4f50-bf24-baa1fc493767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading model or tokenizer: Error(s) in loading state_dict for CustomTransformer:\n",
            "\tsize mismatch for embedding.weight: copying a param with shape torch.Size([32100, 256]) from checkpoint, the shape in current model is torch.Size([32000, 256]).\n",
            "\tsize mismatch for fc_out.weight: copying a param with shape torch.Size([32100, 256]) from checkpoint, the shape in current model is torch.Size([32000, 256]).\n",
            "\tsize mismatch for fc_out.bias: copying a param with shape torch.Size([32100]) from checkpoint, the shape in current model is torch.Size([32000]).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning"
      ],
      "metadata": {
        "id": "3AhmpjuRvLvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "def load_model_and_tokenizer(save_dir=\"t5_small_model\", model_name=\"t5-small\"):\n",
        "    model_path = os.path.join(save_dir, \"t5_small_model.pth\")\n",
        "    tokenizer_path = os.path.join(save_dir, \"t5_small_tokenizer.pkl\")\n",
        "\n",
        "    if not (os.path.exists(model_path) and os.path.exists(tokenizer_path)):\n",
        "        raise FileNotFoundError(f\"Model or tokenizer not found in {save_dir}. Ensure t5_small_model.pth and t5_small_tokenizer.pkl exist.\")\n",
        "\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name, local_files_only=False)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
        "\n",
        "    with open(tokenizer_path, \"rb\") as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    return model.to(device).eval(), tokenizer\n",
        "\n",
        "def summarize_text(model, tokenizer, text, max_length=128, min_length=30, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    text = \"summarize: \" + text\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        max_length=256,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
        "    generated_ids = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        num_beams=4,\n",
        "        length_penalty=1.0,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        model, tokenizer = load_model_and_tokenizer()\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return\n",
        "\n",
        "    input_text = input(\"Enter text to summarize: \").strip()\n",
        "    if not input_text:\n",
        "        print(\"Error: Input text cannot be empty.\")\n",
        "        return\n",
        "\n",
        "    summary = summarize_text(model, tokenizer, input_text)\n",
        "    print(\"\\nInput Text:\", input_text)\n",
        "    print(\"Summary:\", summary)\n",
        "    print(\"Summary Word Count:\", len(summary.split()))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Niz6dV0idHCJ",
        "outputId": "f4144364-896e-421c-ec0a-dd8f6a05cb5f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text to summarize: ### **India: A Land of Diversity and Heritage**  #### **Introduction**  India, the seventh-largest country by land area and the second-most populous nation, is a land of immense diversity, rich history, and vibrant culture. Nestled in South Asia, India is bordered by Pakistan, China, Nepal, Bhutan, Bangladesh, and Myanmar, with the Indian Ocean to the south. It is known for its ancient civilization, vast landscapes, and contributions to global science, art, literature, and philosophy. From the towering Himalayas in the north to the serene backwaters of Kerala in the south, India is a nation that embodies unity in diversity.  This essay explores India's history, geography, culture, economy, governance, challenges, and its role on the global stage.  ---  ## **Historical Background**  ### **Ancient India and Civilization**  India has one of the world's oldest civilizations, dating back to the Indus Valley Civilization (3300–1300 BCE). The cities of Harappa and Mohenjo-Daro exemplify advanced urban planning, with well-structured drainage systems and standardized brick sizes. The Vedic Age (1500–500 BCE) followed, introducing Sanskrit literature, early Hinduism, and the caste system.  During the Mauryan Empire (321–185 BCE), Emperor Ashoka played a significant role in spreading Buddhism across Asia. The Gupta Empire (4th–6th century CE) marked the golden age of Indian culture, advancing in mathematics (introduction of zero), astronomy, literature, and medicine.  ### **Medieval India**  Medieval India witnessed the rise of regional kingdoms and the influence of Islam with the Delhi Sultanate (1206–1526) and later the Mughal Empire (1526–1857). The Mughal rulers, especially Akbar, encouraged religious tolerance and built architectural wonders like the Taj Mahal.  ### **Colonial Era and Independence Struggle**  The British East India Company established control over India in the 18th century, exploiting its resources. The Revolt of 1857, also known as the First War of Independence, was the first major resistance against British rule. However, it was only in the 20th century, under the leadership of Mahatma Gandhi, Jawaharlal Nehru, Subhas Chandra Bose, and other freedom fighters, that India gained independence on **August 15, 1947**. The partition of India led to the creation of Pakistan, resulting in significant communal violence and mass migration.  ---  ## **Geography and Natural Resources**  ### **Diverse Terrain**  India spans 3.28 million square kilometers and has a varied topography. The **Himalayas** form the northern boundary, acting as a natural defense and climate regulator. The **Gangetic plains** are the most fertile regions, supporting India's massive agricultural sector. The **Thar Desert** in Rajasthan and the **Deccan Plateau** in the south showcase the nation's geographic extremes.  ### **Rivers and Climate**  Major rivers like the **Ganga, Yamuna, Brahmaputra, Godavari, Krishna, and Kaveri** are crucial for irrigation, hydroelectricity, and sustenance. India experiences diverse climatic conditions—tropical in the south, temperate in the north, and desert-like in the west.  ### **Natural Resources**  India is rich in minerals such as coal, iron ore, bauxite, and petroleum. The country is also a leading producer of agricultural commodities like rice, wheat, spices, and cotton.  ---  ## **Cultural and Linguistic Diversity**  ### **Languages**  India recognizes **22 official languages**, with Hindi and English serving as the primary means of communication. Over **1,600 dialects** are spoken, reflecting the nation’s linguistic diversity.  ### **Religious Pluralism**  India is the birthplace of **Hinduism, Buddhism, Jainism, and Sikhism**. It is also home to large populations of **Muslims, Christians, Jews, and Zoroastrians**, demonstrating religious harmony despite occasional conflicts.  ### **Festivals and Traditions**  Festivals like **Diwali, Holi, Eid, Christmas, Pongal, Baisakhi**, and **Navratri** symbolize India's cultural vibrancy. Weddings, traditional attire, and culinary diversity further enrich Indian heritage.  ### **Art and Literature**  Indian classical music—**Hindustani and Carnatic**—along with dance forms like **Bharatanatyam, Kathak, Odissi, and Kathakali**, are globally recognized. Literature from ancient texts like the **Vedas, Upanishads, Mahabharata, and Ramayana** to modern works by **Rabindranath Tagore, R.K. Narayan, and Arundhati Roy** showcases India's intellectual richness.  ---  ## **Indian Economy: A Growing Global Power**  ### **Agriculture**  India is one of the largest producers of **rice, wheat, sugarcane, pulses, and dairy products**. Despite its strong agrarian roots, challenges like small landholdings and unpredictable monsoons affect productivity.  ### **Industrial Growth**  India's industrial sector, particularly **automobiles, textiles, steel, and pharmaceuticals**, has witnessed significant growth. The country is the world's largest vaccine producer and a hub for generic medicines.  ### **Information Technology and Services**  India's IT industry, with giants like **TCS, Infosys, and Wipro**, contributes significantly to GDP. The outsourcing sector, particularly in **Bangalore, Hyderabad, and Pune**, has transformed India into a global IT powerhouse.  ### **Infrastructure and Space Exploration**  India's infrastructure development includes **metros, highways, bullet trains, and smart cities**. The **Indian Space Research Organisation (ISRO)** has made remarkable strides, launching missions like **Chandrayaan (Moon) and Mangalyaan (Mars)** at record-low costs.  ---  ## **Government and Politics**  India is the world's **largest democracy**, following a **parliamentary system**. The **President** is the constitutional head, while the **Prime Minister** holds executive power.  ### **Judiciary and Constitution**  The **Indian Constitution**, adopted in 1950, is the world's longest. It guarantees fundamental rights, secularism, and social justice. The Supreme Court ensures legal protection for all citizens.  ### **Challenges in Governance**  * **Corruption**: A major issue, though initiatives like Digital India and Aadhaar have improved transparency. * **Communalism and Casteism**: Though India is largely secular, caste-based and religious conflicts still occur. * **Unemployment and Poverty**: Economic reforms have reduced poverty, but income disparity remains high.  ---  ## **Global Influence and Foreign Policy**  ### **United Nations and International Relations**  India is a key player in global politics, advocating **peace, non-alignment, and South-South cooperation**. It is a member of the **G20, BRICS, and United Nations Security Council (aspiring for a permanent seat)**.  ### **Defense and Nuclear Power**  India has a strong military force and is a **nuclear power**, with a “No First Use” policy. The country is enhancing defense capabilities through projects like **Make in India**.  ### **Trade and Diplomacy**  India has trade relations with **the USA, European Union, China, Russia, and neighboring countries**. The Indian diaspora in the **Middle East, North America, and the UK** significantly contributes to foreign remittances.  ---  ## **Challenges and the Road Ahead**  ### **Economic Inequality**  India, despite being an emerging economy, has significant **wealth disparity**. Policies promoting rural development and skill enhancement can bridge the gap.  ### **Environmental Issues**  Deforestation, air pollution, and water scarcity threaten India's sustainability. Renewable energy initiatives like **solar and wind power** are promising solutions.  ### **Education and Healthcare**  Improving **public healthcare and education** is essential. The **National Education Policy (NEP 2020)** and **Ayushman Bharat scheme** aim to transform these sectors.  ---  ## **Conclusion**  India, with its ancient legacy and modern aspirations, is on the path of progress. As a rapidly growing economy and a cultural powerhouse, India continues to play a crucial role on the world stage. By embracing **technology, education, sustainability, and social justice**, India can achieve unparalleled growth in the 21st century.  India’s journey—from an ancient civilization to a modern global leader—remains an inspiring tale of resilience and ambition. 🚀  ---  Would you like me to add references or more details on a specific section? \n",
            "\n",
            "Input Text: ### **India: A Land of Diversity and Heritage**  #### **Introduction**  India, the seventh-largest country by land area and the second-most populous nation, is a land of immense diversity, rich history, and vibrant culture. Nestled in South Asia, India is bordered by Pakistan, China, Nepal, Bhutan, Bangladesh, and Myanmar, with the Indian Ocean to the south. It is known for its ancient civilization, vast landscapes, and contributions to global science, art, literature, and philosophy. From the towering Himalayas in the north to the serene backwaters of Kerala in the south, India is a nation that embodies unity in diversity.  This essay explores India's history, geography, culture, economy, governance, challenges, and its role on the global stage.  ---  ## **Historical Background**  ### **Ancient India and Civilization**  India has one of the world's oldest civilizations, dating back to the Indus Valley Civilization (3300–1300 BCE). The cities of Harappa and Mohenjo-Daro exemplify advanced urban planning, with well-structured drainage systems and standardized brick sizes. The Vedic Age (1500–500 BCE) followed, introducing Sanskrit literature, early Hinduism, and the caste system.  During the Mauryan Empire (321–185 BCE), Emperor Ashoka played a significant role in spreading Buddhism across Asia. The Gupta Empire (4th–6th century CE) marked the golden age of Indian culture, advancing in mathematics (introduction of zero), astronomy, literature, and medicine.  ### **Medieval India**  Medieval India witnessed the rise of regional kingdoms and the influence of Islam with the Delhi Sultanate (1206–1526) and later the Mughal Empire (1526–1857). The Mughal rulers, especially Akbar, encouraged religious tolerance and built architectural wonders like the Taj Mahal.  ### **Colonial Era and Independence Struggle**  The British East India Company established control over India in the 18th century, exploiting its resources. The Revolt of 1857, also known as the First War of Independence, was the first major resistance against British rule. However, it was only in the 20th century, under the leadership of Mahatma Gandhi, Jawaharlal Nehru, Subhas Chandra Bose, and other freedom fighters, that India gained independence on **August 15, 1947**. The partition of India led to the creation of Pakistan, resulting in significant communal violence and mass migration.  ---  ## **Geography and Natural Resources**  ### **Diverse Terrain**  India spans 3.28 million square kilometers and has a varied topography. The **Himalayas** form the northern boundary, acting as a natural defense and climate regulator. The **Gangetic plains** are the most fertile regions, supporting India's massive agricultural sector. The **Thar Desert** in Rajasthan and the **Deccan Plateau** in the south showcase the nation's geographic extremes.  ### **Rivers and Climate**  Major rivers like the **Ganga, Yamuna, Brahmaputra, Godavari, Krishna, and Kaveri** are crucial for irrigation, hydroelectricity, and sustenance. India experiences diverse climatic conditions—tropical in the south, temperate in the north, and desert-like in the west.  ### **Natural Resources**  India is rich in minerals such as coal, iron ore, bauxite, and petroleum. The country is also a leading producer of agricultural commodities like rice, wheat, spices, and cotton.  ---  ## **Cultural and Linguistic Diversity**  ### **Languages**  India recognizes **22 official languages**, with Hindi and English serving as the primary means of communication. Over **1,600 dialects** are spoken, reflecting the nation’s linguistic diversity.  ### **Religious Pluralism**  India is the birthplace of **Hinduism, Buddhism, Jainism, and Sikhism**. It is also home to large populations of **Muslims, Christians, Jews, and Zoroastrians**, demonstrating religious harmony despite occasional conflicts.  ### **Festivals and Traditions**  Festivals like **Diwali, Holi, Eid, Christmas, Pongal, Baisakhi**, and **Navratri** symbolize India's cultural vibrancy. Weddings, traditional attire, and culinary diversity further enrich Indian heritage.  ### **Art and Literature**  Indian classical music—**Hindustani and Carnatic**—along with dance forms like **Bharatanatyam, Kathak, Odissi, and Kathakali**, are globally recognized. Literature from ancient texts like the **Vedas, Upanishads, Mahabharata, and Ramayana** to modern works by **Rabindranath Tagore, R.K. Narayan, and Arundhati Roy** showcases India's intellectual richness.  ---  ## **Indian Economy: A Growing Global Power**  ### **Agriculture**  India is one of the largest producers of **rice, wheat, sugarcane, pulses, and dairy products**. Despite its strong agrarian roots, challenges like small landholdings and unpredictable monsoons affect productivity.  ### **Industrial Growth**  India's industrial sector, particularly **automobiles, textiles, steel, and pharmaceuticals**, has witnessed significant growth. The country is the world's largest vaccine producer and a hub for generic medicines.  ### **Information Technology and Services**  India's IT industry, with giants like **TCS, Infosys, and Wipro**, contributes significantly to GDP. The outsourcing sector, particularly in **Bangalore, Hyderabad, and Pune**, has transformed India into a global IT powerhouse.  ### **Infrastructure and Space Exploration**  India's infrastructure development includes **metros, highways, bullet trains, and smart cities**. The **Indian Space Research Organisation (ISRO)** has made remarkable strides, launching missions like **Chandrayaan (Moon) and Mangalyaan (Mars)** at record-low costs.  ---  ## **Government and Politics**  India is the world's **largest democracy**, following a **parliamentary system**. The **President** is the constitutional head, while the **Prime Minister** holds executive power.  ### **Judiciary and Constitution**  The **Indian Constitution**, adopted in 1950, is the world's longest. It guarantees fundamental rights, secularism, and social justice. The Supreme Court ensures legal protection for all citizens.  ### **Challenges in Governance**  * **Corruption**: A major issue, though initiatives like Digital India and Aadhaar have improved transparency. * **Communalism and Casteism**: Though India is largely secular, caste-based and religious conflicts still occur. * **Unemployment and Poverty**: Economic reforms have reduced poverty, but income disparity remains high.  ---  ## **Global Influence and Foreign Policy**  ### **United Nations and International Relations**  India is a key player in global politics, advocating **peace, non-alignment, and South-South cooperation**. It is a member of the **G20, BRICS, and United Nations Security Council (aspiring for a permanent seat)**.  ### **Defense and Nuclear Power**  India has a strong military force and is a **nuclear power**, with a “No First Use” policy. The country is enhancing defense capabilities through projects like **Make in India**.  ### **Trade and Diplomacy**  India has trade relations with **the USA, European Union, China, Russia, and neighboring countries**. The Indian diaspora in the **Middle East, North America, and the UK** significantly contributes to foreign remittances.  ---  ## **Challenges and the Road Ahead**  ### **Economic Inequality**  India, despite being an emerging economy, has significant **wealth disparity**. Policies promoting rural development and skill enhancement can bridge the gap.  ### **Environmental Issues**  Deforestation, air pollution, and water scarcity threaten India's sustainability. Renewable energy initiatives like **solar and wind power** are promising solutions.  ### **Education and Healthcare**  Improving **public healthcare and education** is essential. The **National Education Policy (NEP 2020)** and **Ayushman Bharat scheme** aim to transform these sectors.  ---  ## **Conclusion**  India, with its ancient legacy and modern aspirations, is on the path of progress. As a rapidly growing economy and a cultural powerhouse, India continues to play a crucial role on the world stage. By embracing **technology, education, sustainability, and social justice**, India can achieve unparalleled growth in the 21st century.  India’s journey—from an ancient civilization to a modern global leader—remains an inspiring tale of resilience and ambition. 🚀  ---  Would you like me to add references or more details on a specific section?\n",
            "Summary: india is the seventh-largest country by land area and second-most populous nation. it is known for its ancient civilization, vast landscapes, and contributions to global science, art, literature, and philosophy. from the towering Himalayas in the north to the serene backwaters of Kerala in the south, India is a nation that embodies unity in diversity.\n",
            "Summary Word Count: 55\n"
          ]
        }
      ]
    }
  ]
}